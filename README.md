<div>
    <h1 align="left"><img width="128" src="artwork/tidesdb-logo-v0.1.png"></h1>
</div>

TidesDB is a fast and efficient key value storage engine library written in C. The underlying data structure is based on a log-structured merge-tree (LSM-tree).

It is not a full-featured database, but rather a library that can be used to build a database atop of or used as a standalone key-value/column store.

[![Linux Build Status](https://github.com/tidesdb/tidesdb/actions/workflows/build_and_test_tidesdb.yml/badge.svg)](https://github.com/tidesdb/tidesdb/actions/workflows/build_and_test_tidesdb.yml)

## Features
- Data-structures are 99% lock-free with atomic CAS operations. Reads are completely lock-free and scale linearly with CPU cores. Writes use atomic CAS to prepend new versions without locks. Only 2 essential rwlocks remain: `active_txns_lock` for SERIALIZABLE isolation conflict detection and `cf_list_lock` for column family lifecycle management. All hot paths (memtable operations, WAL group commit, comparator lookups, manifest updates, level operations) are completely lock-free. Immutable memtable queue ensures data remains searchable during background flush operations, guaranteeing consistent reads without blocking.
- ACID transactions with full MVCC (Multi-Version Concurrency Control) supporting 5 isolation levels: `read-uncommitted`, `read-committed`, `repeatable-read` `snapshot`, and `serializable`. Each transaction operates on a consistent snapshot with sequence numbers for version ordering. Snapshot isolation provides first-committer-wins conflict detection preventing dirty reads, non-repeatable reads, and lost updates. Serializable isolation implements SSI (Serializable Snapshot Isolation) with read-write conflict detection to prevent all anomalies including write-skew. Transactions support read-your-own-writes semantics, savepoints for partial rollback, and optimistic concurrency control. Lock-free writes with atomic sequence numbers ensure ordering without blocking.
- Multi-column family atomicity guarantees that transactions spanning multiple column families either commit to all or none. Single-CF transactions use per-CF sequence numbers for maximum performance. Multi-CF transactions use global sequence numbers with high-bit flagging (bit 63) to mark entries that require atomic validation. During recovery, multi-CF transactions are only applied if all participating column families have the complete sequence, ensuring true all-or-nothing semantics without coordinator locks or two-phase commit overhead. This enables referential integrity and cross-table consistency for SQL-like workloads.
- Column families provide isolated key-value stores, each with independent configuration, memtables, SSTables, write-ahead logs, etc. Transactions can span multiple column families with per-CF consistent snapshots. Runtime configuration updates allow dynamic tuning of write buffer size, compression, and compaction parameters without restart.
- Bidirectional iterators support seek, forward and backward traversal with heap-based merge-sort across active memtables, immutable memtables, and SSTables. Lock-free iteration with reference counting prevents premature deletion during concurrent operations. Snapshot isolation ensures consistent iteration even during concurrent writes and compactions.
- Efficient seek operations using O(log n) skip list positioning and optional block indexes for prefix boundary to block mapping in SSTables. Block indexes sample every Nth block (configurable `index_sample_ratio`, default 16) by storing the first and last key prefix of sampled blocks along with their file position, creating a sparse index that enables binary search to locate the correct block range for a given key. Configurable prefix length (default 16 bytes) balances index size with seek precision. Lower sample ratios (e.g., 8) create denser indexes for faster seeks at the cost of more memory, while higher ratios (e.g., 32) reduce memory usage.
- Hybrid compaction policy with three modes: full preemptive merge for initial levels (minimize space amplification), dividing merge at configurable dividing level (create partition boundaries), and partitioned merge for deep levels (minimize write amplification). Dynamic Capacity Adjustment (DCA) automatically scales level capacities based on largest level size using the formula C_i = N_L / T^(L-i), ensuring optimal size ratios as data grows. Dynamic level management adds and removes levels on demand. Configurable dividing level offset allows tuning the transition point between aggressive and incremental compaction strategies.
- Durability through write-ahead log (WAL) with sequence numbers for ordering and automatic recovery on startup that reconstructs memtables from persisted logs. Lock-free WAL group commit batches multiple concurrent writes into single disk operations using atomic reservation (threads atomically reserve buffer space via CAS) and CAS-based leader election (first thread to reach buffer capacity becomes leader and performs the flush), maximizing throughput without blocking. Multi-CF transaction metadata is embedded in WAL entries, recording all participating column families for atomic recovery validation. SSTable metadata persistence ensures accurate recovery of min/max keys and entry counts. Immutable memtables remain searchable during flush, preventing data loss windows.
- Concurrent background operations with shared thread pools for flush and compaction. Multiple flush workers process different memtables in parallel without blocking reads or writes. Compaction workers coordinate via per-column-family atomic `is_compacting` flag (CAS-based) to prevent conflicts while allowing parallel compaction across different column families. Manifest updates during compaction require no locks since operations are serialized by design (single flush thread per CF, single compaction per CF via atomic flag, recovery is single-threaded). Adaptive multi-tier backpressure system prevents write stalls: (1) immutable memtable queue depth triggers delays (3+ pending: 1ms, 6+ pending: 5ms, 10+ pending: 20ms), (2) Spooky-style file-count-based backpressure (β trigger at 20 files: 20ms throttle, γ trigger at 36 files: 100ms stall) based on "Spooky: Granular Space-Efficient Compaction for LSM-trees" (SIGMOD 2024), and (3) Level 0 capacity utilization triggers progressive slowdowns (90-95%: 1ms, 95-98%: 5ms, 98-100%: 10ms, >100%: 50ms emergency brake). Compaction is automatically triggered after successful flush operations when Level 0 reaches 4 files (α trigger) or exceeds capacity, ensuring write amplification stays bounded.
- Optional bloom filters provide probabilistic key existence checks to reduce disk reads. Configurable false positive rate per column family. Bloom filters are built during SSTable creation and persisted in metadata blocks.
- Key-value separation (WiscKey-style) with configurable value threshold (default 512 bytes). Small values are stored inline with keys in the klog (key log) for fast access. Large values exceeding the threshold are stored in the vlog (value log) with only an offset stored in the klog entry. This reduces write amplification during compaction since large values are not rewritten with keys. Klog blocks use 64KB default size while vlog blocks use 4KB default size for efficient I/O. Both klog and vlog blocks support optional compression, though WAL entries remain uncompressed for fast writes and recovery. Configurable per column family with runtime updates.
- TTL (time-to-live) support for key-value pairs with automatic expiration. Expired entries are skipped during reads and removed during compaction. Sequence number-based MVCC ensures correct handling of expired versions.
- Custom comparators allow registration of user-defined key comparison functions with context pointers for stateful comparisons. Lock-free comparator registry uses atomic COW (copy-on-write) pattern with CAS for registration (allocate new array, copy existing entries, add new entry, atomically swap pointer) and atomic loads for lookups, eliminating contention on hot read paths. Built-in comparators include memcmp, lexicographic, and more. Comparators are used consistently across skip lists, SSTables, indexes, and merge operations and support reverse order.
- Block manager provides lock-free concurrent I/O using `pread`/`pwrite` for true parallelism where readers never block readers or writers, and writers don't block each other. Atomic file size tracking eliminates syscalls on hot paths. Reference-counted blocks with atomic acquire/release enable safe sharing across threads without locks. Stack-allocated buffers for small writes (< 64KB) avoid heap allocation overhead. xxHash32 checksums with header+footer validation ensure data integrity. Sequential read hints (`posix_fadvise`) optimize OS page cache behavior. Supports up to 4GB blocks with efficient partial reads for key-only scans without loading full values.
- Two-tier caching system for optimal performance:
  - SSTable File Handle Management · Lock-free atomic design manages file descriptor limits without blocking reads. SSTables are opened on-demand during first access and kept open for subsequent reads. Each SSTable tracks `last_access_time` atomically for LRU eviction. Global `num_open_sstables` counter enforces configurable limit (default 512). Background reaper thread wakes every 100ms, updates cached current time (avoiding syscalls on hot paths), checks if limit is reached, and closes 25% of oldest unused SSTables (refcount=1) based on LRU timestamps. Reaper also updates `cached_available_disk_space` every 60 seconds to avoid repeated syscalls during write operations. Newly flushed/compacted SSTables stay open immediately for hot reads. Read path is completely non-blocking with zero retry loops or cache contention. Level-by-level search with early termination optimizes reads by checking bloom filters and range bounds before taking SSTable references, minimizing unnecessary file opens.
  - Block Cache · Lock-free CLOCK cache for hot SSTable blocks with automatic configuration based on CPU count and memory budget. Caches entire deserialized klog blocks (not individual key-value pairs) for maximum efficiency—one cached block serves hundreds of keys, dramatically reducing memory overhead while eliminating disk I/O, decompression, and deserialization for hot data. Uses atomic operations with CLOCK eviction algorithm (second-chance page replacement) for true lock-free concurrent reads and writes without mutexes. Partitioned design (2 partitions per CPU core, up to 128 partitions) minimizes contention with thread-local clock hands. Configurable memory budget (default 256MB) with power-of-2 slot sizing for fast modulo operations. Cache keys use format `klog_path:block_position` to uniquely identify each block (e.g., `/path/to/db/users/L2P3_1336.klog:65536`). Zero-copy API returns direct pointers to cached arena-allocated blocks with reference bit protection preventing eviction during access—eliminates copy overhead and race conditions. Blocks are cached after first read during point lookups and seek operations, remaining valid across transactions. Reference bit tracking gives recently accessed blocks a "second chance" before eviction. Hybrid hash table + circular array design enables O(1) lookups with linear probing (max 64 probes) and efficient CLOCK scanning. Memory efficient and significantly reduces disk I/O, decompression, and deserialization CPU overhead for random read workloads while maintaining thread-safety across concurrent transactions and compaction.
- Shared thread pools for background flush and compaction operations with configurable thread counts at the database level (default 2 flush threads, 2 compaction threads). Work queues distribute tasks across workers for parallel processing. Compaction is automatically triggered after each flush when Level 0 reaches or exceeds its capacity, ensuring write amplification stays bounded.
- Three sync modes · `TDB_SYNC_NONE` for maximum performance (OS-managed flushing), `TDB_SYNC_FULL` for maximum durability (fsync on every write), and `TDB_SYNC_INTERVAL` for balanced performance with periodic syncing at configurable microsecond intervals. Structural operations (flush, compaction, WAL rotation) always enforce durability regardless of sync mode to prevent data loss. Configurable per column family.
- Cross-platform support for Linux, macOS, and Windows on both 32-bit and 64-bit architectures with comprehensive platform abstraction layer. Atomic operations, threading primitives, and file I/O are abstracted for portability.
- Full file portability with explicit little-endian serialization throughout; database files can be copied between any platform (x86, ARM, RISC-V, PowerPC) and architecture (32-bit, 64-bit) without conversion. Fixed-width integer encoding ensures consistent layout.
- Clean C API that returns 0 on success and negative error codes on failure for straightforward error handling. Debug logging with configurable verbosity aids development and troubleshooting.

## Getting Started
To learn more about TidesDB, check out [What is TidesDB?](https://tidesdb.com/getting-started/what-is-tidesdb/).

For building and benchmarking instructions [Building & Benchmarking TidesDB](https://tidesdb.com/reference/building/)

For C usage documentation, see the [TidesDB C Reference](https://tidesdb.com/reference/c/).

## Discord Community
Join the [TidesDB Discord Community](https://discord.gg/tWEmjR66cy) to ask questions, work on development, and discuss the future of TidesDB.

## License
Multiple licenses apply to TidesDB. The primary license is the Mozilla Public License Version 2.0 (TidesDB), while additional licenses apply to the dependencies used in the project.

```
Mozilla Public License Version 2.0 (TidesDB)

-- AND --
BSD 3 Clause (Snappy)
BSD 2 (LZ4)
BSD 2 (xxHash - Yann Collet)
BSD 2 (inih - Ben Hoyt)
BSD (Zstandard)
```
